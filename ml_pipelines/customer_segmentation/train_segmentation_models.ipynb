{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AMATO Production - Customer Segmentation ML Pipeline\n",
        "\n",
        "This notebook trains clustering models for customer segmentation using RFM and behavioral data.\n",
        "\n",
        "**Author:** Data Science Team  \n",
        "**Date:** 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Using project root: /Users/priyankmavani/Desktop/apps/amato\n",
            "‚úÖ Successfully imported utils.s3_utils\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import hdbscan\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Add project root to path for imports\n",
        "# Try multiple possible paths for Jupyter notebook compatibility\n",
        "possible_paths = [\n",
        "    Path.cwd(),  # Current working directory\n",
        "    Path.cwd().parent,  # Parent of current directory\n",
        "    Path.cwd().parent.parent,  # Grandparent of current directory\n",
        "    Path(__file__).parent.parent.parent if '__file__' in globals() else None  # If __file__ exists\n",
        "]\n",
        "\n",
        "# Filter out None values and find the one with utils folder\n",
        "project_root = None\n",
        "for path in possible_paths:\n",
        "    if path and (path / 'utils').exists():\n",
        "        project_root = path\n",
        "        break\n",
        "\n",
        "if project_root is None:\n",
        "    # Fallback: use current directory and hope for the best\n",
        "    project_root = Path.cwd()\n",
        "\n",
        "sys.path.append(str(project_root))\n",
        "print(f\"üîß Using project root: {project_root}\")\n",
        "\n",
        "try:\n",
        "    from utils.s3_utils import get_s3_manager\n",
        "    print(\"‚úÖ Successfully imported utils.s3_utils\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import utils.s3_utils: {e}\")\n",
        "    print(\"üîß Trying alternative import...\")\n",
        "    try:\n",
        "        # Try relative import\n",
        "        sys.path.append('.')\n",
        "        from utils.s3_utils import get_s3_manager\n",
        "        print(\"‚úÖ Successfully imported with relative path\")\n",
        "    except ImportError as e2:\n",
        "        print(f\"‚ùå Alternative import also failed: {e2}\")\n",
        "        raise\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Customer Segmentation Pipeline Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomerSegmentationPipeline:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the Customer Segmentation Pipeline\"\"\"\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.metadata = {}\n",
        "        \n",
        "\n",
        "    \n",
        "    def load_data(self):\n",
        "        \"\"\"Load historical training data for customer segmentation\"\"\"\n",
        "        try:\n",
        "            # Load historical training data from S3\n",
        "            logger.info(\"üîç Loading historical training data from S3...\")\n",
        "            s3_manager = get_s3_manager()\n",
        "            s3_manager.load_training_data_from_s3()\n",
        "            logger.info(\"‚úÖ Historical training data loaded from S3\")\n",
        "\n",
        "            # Now try to load the local file\n",
        "            data_path = 'data_pipelines/unified_dataset/output/unified_customer_dataset.parquet'\n",
        "            if os.path.exists(data_path):\n",
        "                df = pd.read_parquet(data_path)\n",
        "                logger.info(f\"‚úÖ Loaded historical training dataset: {df.shape}\")\n",
        "                return df\n",
        "            else:\n",
        "                logger.error(f\"‚ùå Historical training dataset not found at {data_path}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to load historical training data: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def prepare_features(self, df):\n",
        "        \"\"\"Prepare features for customer segmentation\"\"\"\n",
        "        logger.info(\"üîß Preparing features for segmentation...\")\n",
        "        \n",
        "        # Select RFM and behavioral features\n",
        "        feature_columns = [\n",
        "            'recency_days', 'frequency', 'monetary_value',\n",
        "            'avg_order_value', 'total_orders', 'days_since_first_order',\n",
        "            'customer_lifetime_value', 'avg_days_between_orders',\n",
        "            'order_count_30d', 'order_count_90d', 'order_count_365d',\n",
        "            'revenue_30d', 'revenue_90d', 'revenue_365d'\n",
        "        ]\n",
        "        \n",
        "        # Filter available features\n",
        "        available_features = [col for col in feature_columns if col in df.columns]\n",
        "        \n",
        "        if len(available_features) < 5:\n",
        "            logger.warning(f\"‚ö†Ô∏è  Only {len(available_features)} features available for segmentation\")\n",
        "            \n",
        "        # Create feature matrix\n",
        "        X = df[available_features].copy()\n",
        "        \n",
        "        # Handle missing values\n",
        "        X = X.fillna(X.median())\n",
        "        \n",
        "        # Remove outliers using IQR method\n",
        "        for col in X.columns:\n",
        "            Q1 = X[col].quantile(0.25)\n",
        "            Q3 = X[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            X[col] = X[col].clip(lower_bound, upper_bound)\n",
        "        \n",
        "        logger.info(f\"‚úÖ Prepared {len(X)} customers with {len(X.columns)} features\")\n",
        "        return X, available_features\n",
        "\n",
        "    def run_training_pipeline(self):\n",
        "        \"\"\"Run the complete customer segmentation training pipeline\"\"\"\n",
        "        logger.info(\"üöÄ Starting Customer Segmentation Training Pipeline...\")\n",
        "        \n",
        "        try:\n",
        "            # Load data\n",
        "            df = self.load_data()\n",
        "            if df is None:\n",
        "                raise Exception(\"Failed to load data\")\n",
        "            \n",
        "            # Prepare features\n",
        "            X, feature_names = self.prepare_features(df)\n",
        "            if X is None:\n",
        "                raise Exception(\"Failed to prepare features\")\n",
        "            \n",
        "            # Train K-means model\n",
        "            kmeans_model, kmeans_scaler, kmeans_labels, kmeans_metrics = self.train_kmeans_model(X)\n",
        "            \n",
        "            # Train HDBSCAN model\n",
        "            hdbscan_model, hdbscan_scaler, hdbscan_labels, hdbscan_metrics = self.train_hdbscan_model(X)\n",
        "            \n",
        "            # Save models\n",
        "            output_dir = self.save_models(\n",
        "                kmeans_model, kmeans_scaler, hdbscan_model, hdbscan_scaler,\n",
        "                kmeans_metrics, hdbscan_metrics, feature_names\n",
        "            )\n",
        "            \n",
        "            logger.info(\"=\" * 60)\n",
        "            logger.info(\"üéâ CUSTOMER SEGMENTATION TRAINING COMPLETED!\")\n",
        "            logger.info(\"=\" * 60)\n",
        "            logger.info(f\"üìä Trained 2 models on {len(X)} customers\")\n",
        "            logger.info(f\"üîß Used {len(feature_names)} features\")\n",
        "            logger.info(f\"üíæ Models saved to: {output_dir}\")\n",
        "            \n",
        "            return {\n",
        "                'kmeans': kmeans_model,\n",
        "                'hdbscan': hdbscan_model,\n",
        "                'kmeans_metrics': kmeans_metrics,\n",
        "                'hdbscan_metrics': hdbscan_metrics\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Error in training pipeline: {e}\")\n",
        "            raise\n",
        "\n",
        "    def train_kmeans_model(self, X, n_clusters=5):\n",
        "        \"\"\"Train K-means clustering model\"\"\"\n",
        "        logger.info(f\"üéØ Training K-means model with {n_clusters} clusters...\")\n",
        "        \n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        \n",
        "        # Train K-means model\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
        "        kmeans.fit(X_scaled)\n",
        "        \n",
        "        # Get cluster labels\n",
        "        cluster_labels = kmeans.predict(X_scaled)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        silhouette_avg = silhouette_score(X_scaled, cluster_labels)\n",
        "        calinski_avg = calinski_harabasz_score(X_scaled, cluster_labels)\n",
        "        \n",
        "        logger.info(f\"‚úÖ K-means training completed\")\n",
        "        logger.info(f\"   Silhouette Score: {silhouette_avg:.4f}\")\n",
        "        logger.info(f\"   Calinski-Harabasz Score: {calinski_avg:.4f}\")\n",
        "        \n",
        "        return kmeans, scaler, cluster_labels, {\n",
        "            'silhouette_score': silhouette_avg,\n",
        "            'calinski_harabasz_score': calinski_avg,\n",
        "            'n_clusters': n_clusters\n",
        "        }\n",
        "\n",
        "    def train_hdbscan_model(self, X, min_cluster_size=50, min_samples=10):\n",
        "        \"\"\"Train HDBSCAN clustering model\"\"\"\n",
        "        logger.info(f\"üéØ Training HDBSCAN model...\")\n",
        "        \n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        \n",
        "        # Train HDBSCAN model\n",
        "        hdbscan_model = hdbscan.HDBSCAN(\n",
        "            min_cluster_size=min_cluster_size,\n",
        "            min_samples=min_samples,\n",
        "            metric='euclidean'\n",
        "        )\n",
        "        hdbscan_model.fit(X_scaled)\n",
        "        \n",
        "        # Get cluster labels\n",
        "        cluster_labels = hdbscan_model.labels_\n",
        "        \n",
        "        # Calculate metrics (only for non-noise points)\n",
        "        valid_labels = cluster_labels[cluster_labels != -1]\n",
        "        valid_indices = cluster_labels != -1\n",
        "        \n",
        "        if len(valid_labels) > 1:\n",
        "            silhouette_avg = silhouette_score(X_scaled[valid_indices], valid_labels)\n",
        "            calinski_avg = calinski_harabasz_score(X_scaled[valid_indices], valid_labels)\n",
        "        else:\n",
        "            silhouette_avg = 0\n",
        "            calinski_avg = 0\n",
        "        \n",
        "        n_clusters = len(set(valid_labels))\n",
        "        noise_ratio = (cluster_labels == -1).sum() / len(cluster_labels)\n",
        "        \n",
        "        logger.info(f\"‚úÖ HDBSCAN training completed\")\n",
        "        logger.info(f\"   Clusters found: {n_clusters}\")\n",
        "        logger.info(f\"   Noise ratio: {noise_ratio:.4f}\")\n",
        "        logger.info(f\"   Silhouette Score: {silhouette_avg:.4f}\")\n",
        "        logger.info(f\"   Calinski-Harabasz Score: {calinski_avg:.4f}\")\n",
        "        \n",
        "        return hdbscan_model, scaler, cluster_labels, {\n",
        "            'silhouette_score': silhouette_avg,\n",
        "            'calinski_harabasz_score': calinski_avg,\n",
        "            'n_clusters': n_clusters,\n",
        "            'noise_ratio': noise_ratio\n",
        "        }\n",
        "\n",
        "    def save_models(self, kmeans_model, kmeans_scaler, hdbscan_model, hdbscan_scaler, \n",
        "                    kmeans_metrics, hdbscan_metrics, feature_names):\n",
        "        \"\"\"Save trained models and metadata directly to S3\"\"\"\n",
        "        logger.info(\"üíæ Saving trained models directly to S3...\")\n",
        "        \n",
        "        try:\n",
        "            s3_manager = get_s3_manager()\n",
        "            \n",
        "            # Save K-means model directly to S3\n",
        "            kmeans_success = s3_manager.upload_model_direct(\n",
        "                kmeans_model, 'kmeans_model', 'customer_segmentation', \n",
        "                {'model_type': 'kmeans', 'metrics': kmeans_metrics}\n",
        "            )\n",
        "            \n",
        "            # Save K-means scaler directly to S3\n",
        "            kmeans_scaler_success = s3_manager.upload_model_direct(\n",
        "                kmeans_scaler, 'kmeans_scaler', 'customer_segmentation', \n",
        "                {'model_type': 'scaler', 'for_model': 'kmeans'}\n",
        "            )\n",
        "            \n",
        "            # Save HDBSCAN model directly to S3\n",
        "            hdbscan_success = s3_manager.upload_model_direct(\n",
        "                hdbscan_model, 'hdbscan_model', 'customer_segmentation', \n",
        "                {'model_type': 'hdbscan', 'metrics': hdbscan_metrics}\n",
        "            )\n",
        "            \n",
        "            # Save HDBSCAN scaler directly to S3\n",
        "            hdbscan_scaler_success = s3_manager.upload_model_direct(\n",
        "                hdbscan_scaler, 'hdbscan_scaler', 'customer_segmentation', \n",
        "                {'model_type': 'scaler', 'for_model': 'hdbscan'}\n",
        "            )\n",
        "            \n",
        "            # Save metadata directly to S3\n",
        "            metadata = {\n",
        "                'kmeans': kmeans_metrics,\n",
        "                'hdbscan': hdbscan_metrics,\n",
        "                'feature_names': feature_names,\n",
        "                'training_date': datetime.now().isoformat(),\n",
        "                'model_versions': {\n",
        "                    'kmeans': '1.0',\n",
        "                    'hdbscan': '1.0'\n",
        "                }\n",
        "            }\n",
        "            \n",
        "            metadata_success = s3_manager.upload_bytes_direct(\n",
        "                yaml.dump(metadata, default_flow_style=False).encode('utf-8'),\n",
        "                'models/customer_segmentation/pipeline_report.yaml',\n",
        "                'text/yaml'\n",
        "            )\n",
        "            \n",
        "            # Check success\n",
        "            all_success = all([\n",
        "                kmeans_success, kmeans_scaler_success, \n",
        "                hdbscan_success, hdbscan_scaler_success, metadata_success\n",
        "            ])\n",
        "            \n",
        "            if all_success:\n",
        "                logger.info(\"‚úÖ All models and metadata uploaded directly to S3\")\n",
        "                return \"S3\"\n",
        "            else:\n",
        "                logger.warning(\"‚ö†Ô∏è  Some models failed to upload to S3\")\n",
        "                return \"S3_partial\"\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to upload models to S3: {e}\")\n",
        "            return \"failed\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:üöÄ Starting Customer Segmentation Training Pipeline...\n",
            "INFO:__main__:üîç Loading historical training data from S3...\n",
            "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
            "INFO:utils.s3_utils:Loading historical training data from S3 with smart caching...\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output/recent_customer_dataset.parquet to data_pipelines/unified_dataset/output/recent_customer_dataset.parquet\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output/timeline_datasets_metadata.yaml to data_pipelines/unified_dataset/output/timeline_datasets_metadata.yaml\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output/unified_customer_dataset.parquet to data_pipelines/unified_dataset/output/unified_customer_dataset.parquet\n",
            "INFO:__main__:‚úÖ Historical training data loaded from S3\n",
            "INFO:__main__:‚úÖ Loaded historical training dataset: (8514, 89)\n",
            "INFO:__main__:üîß Preparing features for segmentation...\n",
            "INFO:__main__:‚úÖ Prepared 8514 customers with 5 features\n",
            "INFO:__main__:üéØ Training K-means model with 5 clusters...\n",
            "INFO:__main__:‚úÖ K-means training completed\n",
            "INFO:__main__:   Silhouette Score: 0.2675\n",
            "INFO:__main__:   Calinski-Harabasz Score: 4147.5135\n",
            "INFO:__main__:üéØ Training HDBSCAN model...\n",
            "INFO:__main__:‚úÖ HDBSCAN training completed\n",
            "INFO:__main__:   Clusters found: 8\n",
            "INFO:__main__:   Noise ratio: 0.0980\n",
            "INFO:__main__:   Silhouette Score: 0.0146\n",
            "INFO:__main__:   Calinski-Harabasz Score: 965.2366\n",
            "INFO:__main__:üíæ Saving trained models directly to S3...\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/kmeans_model_20250904_113434.pkl\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/kmeans_model_20250904_113434.pkl\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/kmeans_model_metadata_20250904_113434.yaml\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/kmeans_model_metadata_20250904_113434.yaml\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/kmeans_scaler_20250904_113435.pkl\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/kmeans_scaler_20250904_113435.pkl\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/kmeans_scaler_metadata_20250904_113435.yaml\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/kmeans_scaler_metadata_20250904_113435.yaml\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/hdbscan_model_20250904_113435.pkl\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/hdbscan_model_20250904_113435.pkl\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/hdbscan_model_metadata_20250904_113435.yaml\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/hdbscan_model_metadata_20250904_113435.yaml\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/hdbscan_scaler_20250904_113435.pkl\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/hdbscan_scaler_20250904_113435.pkl\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/hdbscan_scaler_metadata_20250904_113435.yaml\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/amato_pm/models/customer_segmentation/hdbscan_scaler_metadata_20250904_113435.yaml\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/models/customer_segmentation/pipeline_report.yaml\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/models/customer_segmentation/pipeline_report.yaml\n",
            "INFO:__main__:‚úÖ All models and metadata uploaded directly to S3\n",
            "INFO:__main__:============================================================\n",
            "INFO:__main__:üéâ CUSTOMER SEGMENTATION TRAINING COMPLETED!\n",
            "INFO:__main__:============================================================\n",
            "INFO:__main__:üìä Trained 2 models on 8514 customers\n",
            "INFO:__main__:üîß Used 5 features\n",
            "INFO:__main__:üíæ Models saved to: S3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ Customer Segmentation Training completed successfully!\n",
            "üìä K-means: 5 clusters, Silhouette: 0.2675\n",
            "üìä HDBSCAN: 8 clusters, Noise: 0.0980\n",
            "üíæ Models saved and ready for inference!\n"
          ]
        }
      ],
      "source": [
        "# Initialize and run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = CustomerSegmentationPipeline()\n",
        "    results = pipeline.run_training_pipeline()\n",
        "    \n",
        "    print(\"\\nüéâ Customer Segmentation Training completed successfully!\")\n",
        "    print(f\"üìä K-means: {results['kmeans_metrics']['n_clusters']} clusters, Silhouette: {results['kmeans_metrics']['silhouette_score']:.4f}\")\n",
        "    print(f\"üìä HDBSCAN: {results['hdbscan_metrics']['n_clusters']} clusters, Noise: {results['hdbscan_metrics']['noise_ratio']:.4f}\")\n",
        "    print(\"üíæ Models saved and ready for inference!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
