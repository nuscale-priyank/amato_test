{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AMATO Production - Customer Segmentation Batch Inference Pipeline\n",
        "\n",
        "This notebook performs batch inference on customer data using trained segmentation models.\n",
        "\n",
        "**Author:** Data Science Team  \n",
        "**Date:** 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Using project root: /Users/priyankmavani/Desktop/apps/amato\n",
            "‚úÖ Successfully imported utils.s3_utils\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Add project root to path for imports\n",
        "# Try multiple possible paths for Jupyter notebook compatibility\n",
        "possible_paths = [\n",
        "    Path.cwd(),  # Current working directory\n",
        "    Path.cwd().parent,  # Parent of current directory\n",
        "    Path.cwd().parent.parent,  # Grandparent of current directory\n",
        "    Path(__file__).parent.parent.parent if '__file__' in globals() else None  # If __file__ exists\n",
        "]\n",
        "\n",
        "# Filter out None values and find the one with utils folder\n",
        "project_root = None\n",
        "for path in possible_paths:\n",
        "    if path and (path / 'utils').exists():\n",
        "        project_root = path\n",
        "        break\n",
        "\n",
        "if project_root is None:\n",
        "    # Fallback: use current directory and hope for the best\n",
        "    project_root = Path.cwd()\n",
        "\n",
        "sys.path.append(str(project_root))\n",
        "print(f\"üîß Using project root: {project_root}\")\n",
        "\n",
        "try:\n",
        "    from utils.s3_utils import get_s3_manager\n",
        "    print(\"‚úÖ Successfully imported utils.s3_utils\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import utils.s3_utils: {e}\")\n",
        "    print(\"üîß Trying alternative import...\")\n",
        "    try:\n",
        "        # Try relative import\n",
        "        sys.path.append('.')\n",
        "        from utils.s3_utils import get_s3_manager\n",
        "        print(\"‚úÖ Successfully imported with relative path\")\n",
        "    except ImportError as e2:\n",
        "        print(f\"‚ùå Alternative import also failed: {e2}\")\n",
        "        raise\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Customer Segmentation Batch Inference Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CustomerSegmentationBatchInference:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the Customer Segmentation Batch Inference Pipeline\"\"\"\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.metadata = {}\n",
        "        \n",
        "    def load_trained_models(self):\n",
        "        \"\"\"Load trained segmentation models from S3 with timestamped filenames\"\"\"\n",
        "        logger.info(\"üì• Loading trained segmentation models...\")\n",
        "        \n",
        "        try:\n",
        "            s3_manager = get_s3_manager()\n",
        "            \n",
        "            # Create models directory if it doesn't exist\n",
        "            models_dir = 'models/customer_segmentation'\n",
        "            os.makedirs(models_dir, exist_ok=True)\n",
        "            \n",
        "            # Find latest K-means model and scaler\n",
        "            try:\n",
        "                kmeans_models = s3_manager.list_files('amato_pm/models/customer_segmentation/')\n",
        "                kmeans_model_files = [f for f in kmeans_models if 'kmeans_model_' in f and f.endswith('.pkl')]\n",
        "                kmeans_scaler_files = [f for f in kmeans_models if 'kmeans_scaler_' in f and f.endswith('.pkl')]\n",
        "                \n",
        "                if kmeans_model_files and kmeans_scaler_files:\n",
        "                    # Get latest by timestamp (sort by filename which includes timestamp)\n",
        "                    latest_kmeans_model = sorted(kmeans_model_files)[-1]\n",
        "                    latest_kmeans_scaler = sorted(kmeans_scaler_files)[-1]\n",
        "                    \n",
        "                    logger.info(f\"üì• Found latest K-means model: {latest_kmeans_model}\")\n",
        "                    logger.info(f\"üì• Found latest K-means scaler: {latest_kmeans_scaler}\")\n",
        "                    \n",
        "                    # Download latest K-means model\n",
        "                    kmeans_path = f'{models_dir}/kmeans_model.pkl'\n",
        "                    if not os.path.exists(kmeans_path):\n",
        "                        logger.info(\"üì• Downloading latest K-means model from S3...\")\n",
        "                        s3_manager.download_file(latest_kmeans_model, kmeans_path)\n",
        "                        logger.info(\"‚úÖ Downloaded latest K-means model from S3\")\n",
        "                    \n",
        "                    # Download latest K-means scaler\n",
        "                    kmeans_scaler_path = f'{models_dir}/kmeans_scaler.pkl'\n",
        "                    if not os.path.exists(kmeans_scaler_path):\n",
        "                        logger.info(\"üì• Downloading latest K-means scaler from S3...\")\n",
        "                        s3_manager.download_file(latest_kmeans_scaler, kmeans_scaler_path)\n",
        "                        logger.info(\"‚úÖ Downloaded latest K-means scaler from S3\")\n",
        "                    \n",
        "                    # Load K-means model\n",
        "                    if os.path.exists(kmeans_path) and os.path.exists(kmeans_scaler_path):\n",
        "                        self.models['kmeans'] = joblib.load(kmeans_path)\n",
        "                        self.scalers['kmeans'] = joblib.load(kmeans_scaler_path)\n",
        "                        logger.info(\"‚úÖ Loaded K-means model\")\n",
        "                    else:\n",
        "                        logger.warning(\"‚ö†Ô∏è  K-means model not available\")\n",
        "                else:\n",
        "                    logger.warning(\"‚ö†Ô∏è  No K-means models found in S3\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                logger.warning(f\"‚ö†Ô∏è  Failed to find/download K-means models: {e}\")\n",
        "            \n",
        "            # Find latest HDBSCAN model and scaler\n",
        "            try:\n",
        "                hdbscan_model_files = [f for f in kmeans_models if 'hdbscan_model_' in f and f.endswith('.pkl')]\n",
        "                hdbscan_scaler_files = [f for f in kmeans_models if 'hdbscan_scaler_' in f and f.endswith('.pkl')]\n",
        "                \n",
        "                if hdbscan_model_files and hdbscan_scaler_files:\n",
        "                    # Get latest by timestamp\n",
        "                    latest_hdbscan_model = sorted(hdbscan_model_files)[-1]\n",
        "                    latest_hdbscan_scaler = sorted(hdbscan_scaler_files)[-1]\n",
        "                    \n",
        "                    logger.info(f\"üì• Found latest HDBSCAN model: {latest_hdbscan_model}\")\n",
        "                    logger.info(f\"üì• Found latest HDBSCAN scaler: {latest_hdbscan_scaler}\")\n",
        "                    \n",
        "                    # Download latest HDBSCAN model\n",
        "                    hdbscan_path = f'{models_dir}/hdbscan_model.pkl'\n",
        "                    if not os.path.exists(hdbscan_path):\n",
        "                        logger.info(\"üì• Downloading latest HDBSCAN model from S3...\")\n",
        "                        s3_manager.download_file(latest_hdbscan_model, hdbscan_path)\n",
        "                        logger.info(\"‚úÖ Downloaded latest HDBSCAN model from S3\")\n",
        "                    \n",
        "                    # Download latest HDBSCAN scaler\n",
        "                    hdbscan_scaler_path = f'{models_dir}/hdbscan_scaler.pkl'\n",
        "                    if not os.path.exists(hdbscan_scaler_path):\n",
        "                        logger.info(\"üì• Downloading latest HDBSCAN scaler from S3...\")\n",
        "                        s3_manager.download_file(latest_hdbscan_scaler, hdbscan_scaler_path)\n",
        "                        logger.info(\"‚úÖ Downloaded latest HDBSCAN scaler from S3\")\n",
        "                    \n",
        "                    # Load HDBSCAN model\n",
        "                    if os.path.exists(hdbscan_path) and os.path.exists(hdbscan_scaler_path):\n",
        "                        self.models['hdbscan'] = joblib.load(hdbscan_path)\n",
        "                        self.scalers['hdbscan'] = joblib.load(hdbscan_scaler_path)\n",
        "                        logger.info(\"‚úÖ Loaded HDBSCAN model\")\n",
        "                    else:\n",
        "                        logger.warning(\"‚ö†Ô∏è  HDBSCAN model not available\")\n",
        "                else:\n",
        "                    logger.warning(\"‚ö†Ô∏è  No HDBSCAN models found in S3\")\n",
        "                    \n",
        "            except Exception as e:\n",
        "                logger.warning(f\"‚ö†Ô∏è  Failed to find/download HDBSCAN models: {e}\")\n",
        "            \n",
        "            logger.info(f\"‚úÖ Loaded {len(self.models)} models\")\n",
        "            \n",
        "            if len(self.models) == 0:\n",
        "                logger.error(\"‚ùå No models loaded. Please ensure models are available in S3.\")\n",
        "                raise Exception(\"No models available for inference\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to load models: {e}\")\n",
        "            raise\n",
        "    \n",
        "    def load_inference_data(self, data_path=None):\n",
        "        \"\"\"Load recent inference data for customer segmentation\"\"\"\n",
        "        logger.info(\"üìä Loading recent inference data...\")\n",
        "        \n",
        "        try:\n",
        "            # Load recent inference data from S3\n",
        "            logger.info(\"üîç Loading recent inference data from S3...\")\n",
        "            s3_manager = get_s3_manager()\n",
        "            s3_manager.load_inference_data_from_s3()\n",
        "            logger.info(\"‚úÖ Recent inference data loaded from S3\")\n",
        "            \n",
        "            if data_path is None:\n",
        "                data_path = 'data_pipelines/unified_dataset/output/recent_customer_dataset.parquet'\n",
        "            \n",
        "            if os.path.exists(data_path):\n",
        "                df = pd.read_parquet(data_path)\n",
        "                logger.info(f\"‚úÖ Loaded recent inference data: {len(df)} customers\")\n",
        "                return df\n",
        "            else:\n",
        "                logger.error(f\"‚ùå Recent inference data not found at {data_path}\")\n",
        "                return None\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to load recent inference data: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def prepare_features(self, df, model_name):\n",
        "        \"\"\"Prepare features for inference\"\"\"\n",
        "        logger.info(f\"üîß Preparing features for {model_name} inference...\")\n",
        "        \n",
        "        # Select RFM and behavioral features\n",
        "        feature_columns = [\n",
        "            'recency_days', 'frequency', 'monetary_value',\n",
        "            'avg_order_value', 'total_orders', 'days_since_first_order',\n",
        "            'customer_lifetime_value', 'avg_days_between_orders',\n",
        "            'order_count_30d', 'order_count_90d', 'order_count_365d',\n",
        "            'revenue_30d', 'revenue_90d', 'revenue_365d'\n",
        "        ]\n",
        "        \n",
        "        # Filter available features\n",
        "        available_features = [col for col in feature_columns if col in df.columns]\n",
        "        \n",
        "        if len(available_features) < 5:\n",
        "            logger.warning(f\"‚ö†Ô∏è  Only {len(available_features)} features available for {model_name}\")\n",
        "            \n",
        "        # Create feature matrix\n",
        "        X = df[available_features].copy()\n",
        "        \n",
        "        # Handle missing values\n",
        "        X = X.fillna(X.median())\n",
        "        \n",
        "        # Remove outliers using IQR method\n",
        "        for col in X.columns:\n",
        "            Q1 = X[col].quantile(0.25)\n",
        "            Q3 = X[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            X[col] = X[col].clip(lower_bound, upper_bound)\n",
        "        \n",
        "        logger.info(f\"‚úÖ Prepared {len(X)} customers for {model_name}\")\n",
        "        return X\n",
        "    \n",
        "    def perform_kmeans_inference(self, df_features):\n",
        "        \"\"\"Perform K-means inference\"\"\"\n",
        "        logger.info(\"üéØ Performing kmeans inference...\")\n",
        "        \n",
        "        if 'kmeans' not in self.models:\n",
        "            logger.error(\"‚ùå K-means model not loaded\")\n",
        "            return None\n",
        "        \n",
        "        # Scale features\n",
        "        X_scaled = self.scalers['kmeans'].transform(df_features)\n",
        "        \n",
        "        # Predict clusters\n",
        "        cluster_labels = self.models['kmeans'].predict(X_scaled)\n",
        "        \n",
        "        # Create results dataframe\n",
        "        results = df_features.copy()\n",
        "        results['kmeans_segment'] = cluster_labels\n",
        "        results['kmeans_segment_type'] = results['kmeans_segment'].map({\n",
        "            0: 'Low Value', 1: 'Medium Value', 2: 'High Value', 3: 'Premium', 4: 'VIP'\n",
        "        })\n",
        "        \n",
        "        logger.info(f\"‚úÖ kmeans inference completed: {len(results)} predictions\")\n",
        "        return results\n",
        "    \n",
        "    def perform_hdbscan_inference(self, df_features):\n",
        "        \"\"\"Perform HDBSCAN inference\"\"\"\n",
        "        logger.info(\"üéØ Performing hdbscan inference...\")\n",
        "        \n",
        "        if 'hdbscan' not in self.models:\n",
        "            logger.error(\"‚ùå HDBSCAN model not loaded\")\n",
        "            return None\n",
        "        \n",
        "        # Scale features\n",
        "        X_scaled = self.scalers['hdbscan'].transform(df_features)\n",
        "        \n",
        "        # For HDBSCAN, we need to use fit_predict or get the labels from the fitted model\n",
        "        # Since this is a pre-trained model, we'll use the stored labels approach\n",
        "        # or create a new clustering based on the existing model parameters\n",
        "        \n",
        "        # Get the HDBSCAN model\n",
        "        hdbscan_model = self.models['hdbscan']\n",
        "        \n",
        "        # Check if the model has been fitted and has labels\n",
        "        if hasattr(hdbscan_model, 'labels_') and hdbscan_model.labels_ is not None:\n",
        "            # If model was already fitted, we need to create new clusters for new data\n",
        "            # For inference, we'll use the model's parameters to create new clusters\n",
        "            logger.info(\"üìä Creating new HDBSCAN clusters for inference data...\")\n",
        "            \n",
        "            # Create a new HDBSCAN instance with the same parameters\n",
        "            from hdbscan import HDBSCAN\n",
        "            new_hdbscan = HDBSCAN(\n",
        "                min_cluster_size=hdbscan_model.min_cluster_size,\n",
        "                min_samples=hdbscan_model.min_samples,\n",
        "                metric=hdbscan_model.metric,\n",
        "                cluster_selection_method=hdbscan_model.cluster_selection_method,\n",
        "                cluster_selection_epsilon=hdbscan_model.cluster_selection_epsilon,\n",
        "                alpha=hdbscan_model.alpha\n",
        "            )\n",
        "            \n",
        "            # Fit and predict on new data\n",
        "            cluster_labels = new_hdbscan.fit_predict(X_scaled)\n",
        "        else:\n",
        "            # Use fit_predict for new data\n",
        "            cluster_labels = hdbscan_model.fit_predict(X_scaled)\n",
        "        \n",
        "        # Create results dataframe\n",
        "        results = df_features.copy()\n",
        "        results['hdbscan_segment'] = cluster_labels\n",
        "        \n",
        "        # Map cluster labels to meaningful names\n",
        "        unique_clusters = sorted(set(cluster_labels))\n",
        "        cluster_names = {}\n",
        "        for i, cluster_id in enumerate(unique_clusters):\n",
        "            if cluster_id == -1:\n",
        "                cluster_names[cluster_id] = 'Noise'\n",
        "            else:\n",
        "                cluster_names[cluster_id] = f'Segment_{i}'\n",
        "        \n",
        "        results['hdbscan_segment_type'] = results['hdbscan_segment'].map(cluster_names)\n",
        "        \n",
        "        logger.info(f\"‚úÖ hdbscan inference completed: {len(results)} predictions\")\n",
        "        return results\n",
        "    \n",
        "    def save_inference_results(self, results, model_name):\n",
        "        \"\"\"Save inference results directly to S3\"\"\"\n",
        "        logger.info(f\"üíæ Saving {model_name} inference results...\")\n",
        "        \n",
        "        try:\n",
        "            s3_manager = get_s3_manager()\n",
        "            \n",
        "            # Save results directly to S3\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            results_key = f'models/customer_segmentation/inference_results/{model_name}_inference_results_{timestamp}.parquet'\n",
        "            \n",
        "            # Convert to parquet bytes and upload\n",
        "            results_bytes = results.to_parquet(index=False)\n",
        "            results_success = s3_manager.upload_bytes_direct(\n",
        "                results_bytes, results_key, 'application/octet-stream'\n",
        "            )\n",
        "            \n",
        "            # Generate and save report\n",
        "            report = self.generate_inference_report(results, model_name)\n",
        "            report_key = f'models/customer_segmentation/inference_results/{model_name}_inference_report_{timestamp}.yaml'\n",
        "            \n",
        "            report_success = s3_manager.upload_bytes_direct(\n",
        "                yaml.dump(report, default_flow_style=False).encode('utf-8'),\n",
        "                report_key, 'text/yaml'\n",
        "            )\n",
        "            \n",
        "            if results_success and report_success:\n",
        "                logger.info(f\"‚úÖ {model_name} results uploaded directly to S3\")\n",
        "                return results_key, report_key\n",
        "            else:\n",
        "                logger.warning(f\"‚ö†Ô∏è  Some {model_name} results failed to upload to S3\")\n",
        "                return None, None\n",
        "                \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to save {model_name} results: {e}\")\n",
        "            return None, None\n",
        "    \n",
        "    def generate_inference_report(self, results, model_name):\n",
        "        \"\"\"Generate inference report\"\"\"\n",
        "        logger.info(f\"üìã Generating {model_name} inference report...\")\n",
        "        \n",
        "        segment_col = f'{model_name}_segment'\n",
        "        type_col = f'{model_name}_segment_type'\n",
        "        \n",
        "        report = {\n",
        "            'model_name': model_name,\n",
        "            'inference_date': datetime.now().isoformat(),\n",
        "            'total_customers': len(results),\n",
        "            'segment_distribution': results[segment_col].value_counts().to_dict(),\n",
        "            'segment_type_distribution': results[type_col].value_counts().to_dict(),\n",
        "            'feature_summary': {\n",
        "                'total_features': len(results.columns),\n",
        "                'numeric_features': len(results.select_dtypes(include=[np.number]).columns),\n",
        "                'categorical_features': len(results.select_dtypes(include=['object']).columns)\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return report\n",
        "    \n",
        "    def create_inference_visualizations(self, results, model_name):\n",
        "        \"\"\"Create inference visualizations and upload directly to S3\"\"\"\n",
        "        logger.info(f\"üìä Creating {model_name} inference visualizations...\")\n",
        "        \n",
        "        try:\n",
        "            s3_manager = get_s3_manager()\n",
        "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "            \n",
        "            # Segment distribution\n",
        "            fig1 = px.bar(\n",
        "                x=results[f'{model_name}_segment'].value_counts().index,\n",
        "                y=results[f'{model_name}_segment'].value_counts().values,\n",
        "                title=f'{model_name.upper()} Segment Distribution',\n",
        "                labels={'x': 'Segment ID', 'y': 'Customer Count'}\n",
        "            )\n",
        "            \n",
        "            # Segment type distribution\n",
        "            fig2 = px.pie(\n",
        "                values=results[f'{model_name}_segment_type'].value_counts().values,\n",
        "                names=results[f'{model_name}_segment_type'].value_counts().index,\n",
        "                title=f'{model_name.upper()} Segment Type Distribution'\n",
        "            )\n",
        "            \n",
        "            # Upload visualizations directly to S3\n",
        "            html1_key = f'models/customer_segmentation/inference_results/{model_name}_segment_distribution_{timestamp}.html'\n",
        "            html2_key = f'models/customer_segmentation/inference_results/{model_name}_segment_types_{timestamp}.html'\n",
        "            \n",
        "            # Convert figures to HTML and upload\n",
        "            html1_bytes = fig1.to_html().encode('utf-8')\n",
        "            html2_bytes = fig2.to_html().encode('utf-8')\n",
        "            \n",
        "            s3_manager.upload_bytes_direct(html1_bytes, html1_key, 'text/html')\n",
        "            s3_manager.upload_bytes_direct(html2_bytes, html2_key, 'text/html')\n",
        "            \n",
        "            logger.info(f\"‚úÖ {model_name} visualizations uploaded directly to S3\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to create {model_name} visualizations: {e}\")\n",
        "    \n",
        "    def run_batch_inference(self, data_path=None, models=None):\n",
        "        \"\"\"Run batch inference for all models\"\"\"\n",
        "        logger.info(\"üöÄ Starting Customer Segmentation Batch Inference...\")\n",
        "        \n",
        "        try:\n",
        "            # Load models\n",
        "            self.load_trained_models()\n",
        "            \n",
        "            # Load data\n",
        "            df = self.load_inference_data(data_path)\n",
        "            if df is None:\n",
        "                raise Exception(\"Failed to load inference data\")\n",
        "            \n",
        "            # Determine which models to run\n",
        "            if models is None:\n",
        "                models = list(self.models.keys())\n",
        "            \n",
        "            all_results = {}\n",
        "            \n",
        "            for model_name in models:\n",
        "                if model_name not in self.models:\n",
        "                    logger.warning(f\"‚ö†Ô∏è Model {model_name} not found, skipping...\")\n",
        "                    continue\n",
        "                \n",
        "                # Prepare features\n",
        "                df_features = self.prepare_features(df, model_name)\n",
        "                \n",
        "                \n",
        "                if df_features is None or len(df_features) == 0:\n",
        "                    logger.warning(f\"‚ö†Ô∏è  No features prepared for {model_name}, skipping...\")\n",
        "                    continue\n",
        "                \n",
        "                # Perform inference\n",
        "                if model_name == 'kmeans':\n",
        "                    results = self.perform_kmeans_inference(df_features)\n",
        "                elif model_name == 'hdbscan':\n",
        "                    results = self.perform_hdbscan_inference(df_features)\n",
        "                else:\n",
        "                    logger.warning(f\"‚ö†Ô∏è  Unknown model: {model_name}\")\n",
        "                    continue\n",
        "                \n",
        "                if results is not None:\n",
        "                    # Save results\n",
        "                    results_file, report_file = self.save_inference_results(results, model_name)\n",
        "                    \n",
        "                    # Create visualizations\n",
        "                    self.create_inference_visualizations(results, model_name)\n",
        "                    \n",
        "                    all_results[model_name] = results\n",
        "                    \n",
        "                    logger.info(f\"‚úÖ {model_name} batch inference completed\")\n",
        "            \n",
        "            logger.info(\"=\" * 60)\n",
        "            logger.info(\"üéâ BATCH INFERENCE COMPLETED!\")\n",
        "            logger.info(\"=\" * 60)\n",
        "            logger.info(f\"üìä Processed {len(df)} customers\")\n",
        "            logger.info(f\"üéØ Ran inference for {len(all_results)} models\")\n",
        "            \n",
        "            return all_results\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Error in batch inference: {e}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:üöÄ Starting Customer Segmentation Batch Inference...\n",
            "INFO:__main__:üì• Loading trained segmentation models...\n",
            "INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials\n",
            "INFO:__main__:üì• Found latest K-means model: amato_pm/models/customer_segmentation/kmeans_model_20250904_113434.pkl\n",
            "INFO:__main__:üì• Found latest K-means scaler: amato_pm/models/customer_segmentation/kmeans_scaler_20250904_113435.pkl\n",
            "INFO:__main__:‚úÖ Loaded K-means model\n",
            "INFO:__main__:üì• Found latest HDBSCAN model: amato_pm/models/customer_segmentation/hdbscan_model_20250904_113435.pkl\n",
            "INFO:__main__:üì• Found latest HDBSCAN scaler: amato_pm/models/customer_segmentation/hdbscan_scaler_20250904_113435.pkl\n",
            "INFO:__main__:‚úÖ Loaded HDBSCAN model\n",
            "INFO:__main__:‚úÖ Loaded 2 models\n",
            "INFO:__main__:üìä Loading recent inference data...\n",
            "INFO:__main__:üîç Loading recent inference data from S3...\n",
            "INFO:utils.s3_utils:Loading recent inference data from S3 with smart caching (last 3 months)...\n",
            "INFO:utils.s3_utils:Loading data newer than 2025-06-06\n",
            "INFO:utils.s3_utils:File unchanged, skipping: amato_pm/data_pipelines/unified_dataset/output/recent_customer_dataset.parquet\n",
            "INFO:utils.s3_utils:File unchanged, skipping: amato_pm/data_pipelines/unified_dataset/output/timeline_datasets_metadata.yaml\n",
            "INFO:utils.s3_utils:File unchanged, skipping: amato_pm/data_pipelines/unified_dataset/output/unified_customer_dataset.parquet\n",
            "INFO:__main__:‚úÖ Recent inference data loaded from S3\n",
            "INFO:__main__:‚úÖ Loaded recent inference data: 1123 customers\n",
            "INFO:__main__:üîß Preparing features for kmeans inference...\n",
            "INFO:__main__:‚úÖ Prepared 1123 customers for kmeans\n",
            "INFO:__main__:üéØ Performing kmeans inference...\n",
            "INFO:__main__:‚úÖ kmeans inference completed: 1123 predictions\n",
            "INFO:__main__:üíæ Saving kmeans inference results...\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/kmeans_inference_results_20250904_113516.parquet\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/kmeans_inference_results_20250904_113516.parquet\n",
            "INFO:__main__:üìã Generating kmeans inference report...\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/kmeans_inference_report_20250904_113516.yaml\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/kmeans_inference_report_20250904_113516.yaml\n",
            "INFO:__main__:‚úÖ kmeans results uploaded directly to S3\n",
            "INFO:__main__:üìä Creating kmeans inference visualizations...\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/kmeans_segment_distribution_20250904_113517.html\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/kmeans_segment_distribution_20250904_113517.html\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/kmeans_segment_types_20250904_113517.html\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/kmeans_segment_types_20250904_113517.html\n",
            "INFO:__main__:‚úÖ kmeans visualizations uploaded directly to S3\n",
            "INFO:__main__:‚úÖ kmeans batch inference completed\n",
            "INFO:__main__:üîß Preparing features for hdbscan inference...\n",
            "INFO:__main__:‚úÖ Prepared 1123 customers for hdbscan\n",
            "INFO:__main__:üéØ Performing hdbscan inference...\n",
            "INFO:__main__:üìä Creating new HDBSCAN clusters for inference data...\n",
            "INFO:__main__:‚úÖ hdbscan inference completed: 1123 predictions\n",
            "INFO:__main__:üíæ Saving hdbscan inference results...\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/hdbscan_inference_results_20250904_113520.parquet\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/hdbscan_inference_results_20250904_113520.parquet\n",
            "INFO:__main__:üìã Generating hdbscan inference report...\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/hdbscan_inference_report_20250904_113520.yaml\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/hdbscan_inference_report_20250904_113520.yaml\n",
            "INFO:__main__:‚úÖ hdbscan results uploaded directly to S3\n",
            "INFO:__main__:üìä Creating hdbscan inference visualizations...\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/hdbscan_segment_distribution_20250904_113520.html\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/hdbscan_segment_distribution_20250904_113520.html\n",
            "INFO:utils.s3_utils:Uploading bytes directly to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/hdbscan_segment_types_20250904_113520.html\n",
            "INFO:utils.s3_utils:Successfully uploaded bytes to s3://nuscale-data-services-public/models/customer_segmentation/inference_results/hdbscan_segment_types_20250904_113520.html\n",
            "INFO:__main__:‚úÖ hdbscan visualizations uploaded directly to S3\n",
            "INFO:__main__:‚úÖ hdbscan batch inference completed\n",
            "INFO:__main__:============================================================\n",
            "INFO:__main__:üéâ BATCH INFERENCE COMPLETED!\n",
            "INFO:__main__:============================================================\n",
            "INFO:__main__:üìä Processed 1123 customers\n",
            "INFO:__main__:üéØ Ran inference for 2 models\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ Customer Segmentation Batch Inference completed successfully!\n",
            "üìä Results saved to models/customer_segmentation/inference_results/\n",
            "üìà Ready for business analysis!\n"
          ]
        }
      ],
      "source": [
        "# Initialize and run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    inference = CustomerSegmentationBatchInference()\n",
        "    results = inference.run_batch_inference()\n",
        "    \n",
        "    print(\"\\nüéâ Customer Segmentation Batch Inference completed successfully!\")\n",
        "    print(f\"üìä Results saved to models/customer_segmentation/inference_results/\")\n",
        "    print(\"üìà Ready for business analysis!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
