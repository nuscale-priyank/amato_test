{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AMATO Production - Journey Simulation ML Pipeline\n",
        "\n",
        "This notebook trains journey simulation models for customer journey stage prediction and conversion prediction.\n",
        "\n",
        "**Author:** Data Science Team  \n",
        "**Date:** 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Using project root: /Users/priyankmavani/Desktop/apps/amato\n",
            "‚úÖ Successfully imported utils.s3_utils\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yaml\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Add project root to path for imports\n",
        "# Try multiple possible paths for Jupyter notebook compatibility\n",
        "possible_paths = [\n",
        "    Path.cwd(),  # Current working directory\n",
        "    Path.cwd().parent,  # Parent of current directory\n",
        "    Path.cwd().parent.parent,  # Grandparent of current directory\n",
        "    Path(__file__).parent.parent.parent if '__file__' in globals() else None  # If __file__ exists\n",
        "]\n",
        "\n",
        "# Filter out None values and find the one with utils folder\n",
        "project_root = None\n",
        "for path in possible_paths:\n",
        "    if path and (path / 'utils').exists():\n",
        "        project_root = path\n",
        "        break\n",
        "\n",
        "if project_root is None:\n",
        "    # Fallback: use current directory and hope for the best\n",
        "    project_root = Path.cwd()\n",
        "\n",
        "sys.path.append(str(project_root))\n",
        "print(f\"üîß Using project root: {project_root}\")\n",
        "\n",
        "try:\n",
        "    from utils.s3_utils import get_s3_manager\n",
        "    print(\"‚úÖ Successfully imported utils.s3_utils\")\n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Failed to import utils.s3_utils: {e}\")\n",
        "    print(\"üîß Trying alternative import...\")\n",
        "    try:\n",
        "        # Try relative import\n",
        "        sys.path.append('.')\n",
        "        from utils.s3_utils import get_s3_manager\n",
        "        print(\"‚úÖ Successfully imported with relative path\")\n",
        "    except ImportError as e2:\n",
        "        print(f\"‚ùå Alternative import also failed: {e2}\")\n",
        "        raise\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Journey Simulation Pipeline Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class JourneySimulationPipeline:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.metadata = {}\n",
        "        \n",
        "    def load_data(self):\n",
        "        \"\"\"Load customer data for journey simulation from S3\"\"\"\n",
        "        try:\n",
        "            # Load historical training data from S3\n",
        "            logger.info(\"ÔøΩÔøΩ Loading historical training data from S3...\")\n",
        "            s3_manager = get_s3_manager()\n",
        "            s3_manager.load_inference_data_from_s3()\n",
        "            logger.info(\"‚úÖ Historical training data loaded from S3\")\n",
        "            \n",
        "            # Load the historical dataset\n",
        "            data_path = 'data_pipelines/unified_dataset/output/unified_customer_dataset.parquet'\n",
        "            if os.path.exists(data_path):\n",
        "                df = pd.read_parquet(data_path)\n",
        "                logger.info(f\"‚úÖ Loaded historical training dataset: {df.shape}\")\n",
        "                return df\n",
        "            else:\n",
        "                logger.error(f\"‚ùå Historical training dataset not found at {data_path}\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Failed to load data: {e}\")\n",
        "            return None\n",
        "    \n",
        "    def prepare_features(self, df, target_col):\n",
        "        \"\"\"Prepare features for journey simulation\"\"\"\n",
        "        logger.info(f\"ÔøΩÔøΩ Preparing features for {target_col} prediction...\")\n",
        "        \n",
        "        # Select features based on target\n",
        "        if target_col == 'journey_stage':\n",
        "            feature_columns = [\n",
        "                'recency_days', 'frequency', 'monetary_value',\n",
        "                'avg_order_value', 'total_orders', 'days_since_first_order',\n",
        "                'customer_lifetime_value', 'avg_days_between_orders',\n",
        "                'order_count_30d', 'order_count_90d', 'order_count_365d',\n",
        "                'revenue_30d', 'revenue_90d', 'revenue_365d'\n",
        "            ]\n",
        "        elif target_col == 'conversion':\n",
        "            feature_columns = [\n",
        "                'recency_days', 'frequency', 'monetary_value',\n",
        "                'avg_order_value', 'total_orders', 'days_since_first_order',\n",
        "                'customer_lifetime_value', 'avg_days_between_orders',\n",
        "                'order_count_30d', 'order_count_90d', 'order_count_365d',\n",
        "                'revenue_30d', 'revenue_90d', 'revenue_365d'\n",
        "            ]\n",
        "        else:\n",
        "            logger.error(f\"‚ùå Unknown target column: {target_col}\")\n",
        "            return None, None, None\n",
        "        \n",
        "        # Filter available features\n",
        "        available_features = [col for col in feature_columns if col in df.columns]\n",
        "        \n",
        "        if len(available_features) < 5:\n",
        "            logger.warning(f\"‚ö†Ô∏è  Only {len(available_features)} features available for {target_col}\")\n",
        "            \n",
        "        # Create feature matrix\n",
        "        X = df[available_features].copy()\n",
        "        \n",
        "        # Create synthetic target variables based on available data\n",
        "        if target_col == 'journey_stage':\n",
        "            # Create journey stage target based on customer behavior\n",
        "            y = df.apply(lambda row: \n",
        "                min(4, max(0, int(row['frequency'] / 2))), axis=1)\n",
        "        elif target_col == 'conversion':\n",
        "            # Create conversion target based on engagement\n",
        "            # Calculate median of the entire column first\n",
        "            monetary_median = df['monetary_value'].median()\n",
        "            y = df.apply(lambda row: \n",
        "                1 if row['monetary_value'] > monetary_median else 0, axis=1)\n",
        "        else:\n",
        "            y = None\n",
        "        \n",
        "        # Handle missing values\n",
        "        X = X.fillna(X.median())\n",
        "        if y is not None:\n",
        "            y = y.fillna(y.median())\n",
        "        \n",
        "        # Remove outliers using IQR method\n",
        "        for col in X.columns:\n",
        "            Q1 = X[col].quantile(0.25)\n",
        "            Q3 = X[col].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "            lower_bound = Q1 - 1.5 * IQR\n",
        "            upper_bound = Q3 + 1.5 * IQR\n",
        "            X[col] = X[col].clip(lower_bound, upper_bound)\n",
        "        \n",
        "        logger.info(f\"‚úÖ Prepared {len(X)} customers with {len(X.columns)} features for {target_col}\")\n",
        "        return X, y, available_features\n",
        "    \n",
        "    def train_journey_stage_model(self, X, y, n_estimators=100, max_depth=10):\n",
        "        \"\"\"Train journey stage prediction model\"\"\"\n",
        "        logger.info(f\"üéØ Training journey stage prediction model...\")\n",
        "        \n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        \n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        # Train Random Forest model\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        \n",
        "        # Cross-validation score\n",
        "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "        \n",
        "        logger.info(f\"‚úÖ Journey stage prediction training completed\")\n",
        "        logger.info(f\"   Accuracy: {accuracy:.4f}\")\n",
        "        logger.info(f\"   F1 Score: {f1:.4f}\")\n",
        "        logger.info(f\"   CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "        \n",
        "        return model, scaler, {\n",
        "            'accuracy': accuracy,\n",
        "            'f1_score': f1,\n",
        "            'cv_accuracy_mean': cv_scores.mean(),\n",
        "            'cv_accuracy_std': cv_scores.std(),\n",
        "            'n_estimators': n_estimators,\n",
        "            'max_depth': max_depth\n",
        "        }\n",
        "    \n",
        "    def train_conversion_model(self, X, y, n_estimators=100, max_depth=10):\n",
        "        \"\"\"Train conversion prediction model\"\"\"\n",
        "        logger.info(f\"üéØ Training conversion prediction model...\")\n",
        "        \n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        \n",
        "        # Scale features\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        # Train Random Forest model\n",
        "        model = RandomForestClassifier(\n",
        "            n_estimators=n_estimators,\n",
        "            max_depth=max_depth,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "        \n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        \n",
        "        # Cross-validation score\n",
        "        cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
        "        \n",
        "        logger.info(f\"‚úÖ Conversion prediction training completed\")\n",
        "        logger.info(f\"   Accuracy: {accuracy:.4f}\")\n",
        "        logger.info(f\"   F1 Score: {f1:.4f}\")\n",
        "        logger.info(f\"   CV Accuracy: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "        \n",
        "        return model, scaler, {\n",
        "            'accuracy': accuracy,\n",
        "            'f1_score': f1,\n",
        "            'cv_accuracy_mean': cv_scores.mean(),\n",
        "            'cv_accuracy_std': cv_scores.std(),\n",
        "            'n_estimators': n_estimators,\n",
        "            'max_depth': max_depth\n",
        "        }\n",
        "    \n",
        "    def save_models(self, journey_model, journey_scaler, conversion_model, conversion_scaler, \n",
        "                    journey_metrics, conversion_metrics, feature_names):\n",
        "        \"\"\"Save trained models and metadata\"\"\"\n",
        "        logger.info(\"üíæ Saving trained models...\")\n",
        "        \n",
        "        # Create output directory\n",
        "        output_dir = 'models/journey_simulation'\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        \n",
        "        # Save journey stage model\n",
        "        journey_path = os.path.join(output_dir, 'journey_stage_model.pkl')\n",
        "        joblib.dump(journey_model, journey_path)\n",
        "        \n",
        "        # Save journey stage scaler\n",
        "        journey_scaler_path = os.path.join(output_dir, 'journey_stage_scaler.pkl')\n",
        "        joblib.dump(journey_scaler, journey_scaler_path)\n",
        "        \n",
        "        # Save conversion model\n",
        "        conversion_path = os.path.join(output_dir, 'conversion_prediction_model.pkl')\n",
        "        joblib.dump(conversion_model, conversion_path)\n",
        "        \n",
        "        # Save conversion scaler\n",
        "        conversion_scaler_path = os.path.join(output_dir, 'conversion_prediction_scaler.pkl')\n",
        "        joblib.dump(conversion_scaler, conversion_scaler_path)\n",
        "        \n",
        "        # Save metadata\n",
        "        metadata = {\n",
        "            'journey_stage': journey_metrics,\n",
        "            'conversion': conversion_metrics,\n",
        "            'feature_names': feature_names,\n",
        "            'training_date': datetime.now().isoformat(),\n",
        "            'model_versions': {\n",
        "                'journey_stage': '1.0',\n",
        "                'conversion': '1.0'\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        metadata_path = os.path.join(output_dir, 'pipeline_report.yaml')\n",
        "        with open(metadata_path, 'w') as f:\n",
        "            yaml.dump(metadata, f, default_flow_style=False)\n",
        "        \n",
        "        # Upload to S3\n",
        "        try:\n",
        "            s3_manager = get_s3_manager()\n",
        "            s3_manager.upload_file(journey_path, \"amato_pm/models/journey_simulation\")\n",
        "            s3_manager.upload_file(journey_scaler_path, \"amato_pm/models/journey_simulation\")\n",
        "            s3_manager.upload_file(conversion_path, \"amato_pm/models/journey_simulation\")\n",
        "            s3_manager.upload_file(conversion_scaler_path, \"amato_pm/models/journey_simulation\")\n",
        "            s3_manager.upload_file(metadata_path, \"amato_pm/models/journey_simulation\")\n",
        "            logger.info(\"‚úÖ Models uploaded to S3\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"‚ö†Ô∏è  Failed to upload models to S3: {e}\")\n",
        "        \n",
        "        logger.info(f\"‚úÖ Models saved to {output_dir}\")\n",
        "        return output_dir\n",
        "    \n",
        "    def run_training_pipeline(self):\n",
        "        \"\"\"Run the complete journey simulation training pipeline\"\"\"\n",
        "        logger.info(\"üöÄ Starting Journey Simulation Training Pipeline...\")\n",
        "        \n",
        "        try:\n",
        "            # Load data\n",
        "            df = self.load_data()\n",
        "            if df is None:\n",
        "                raise Exception(\"Failed to load data\")\n",
        "            \n",
        "            # Prepare features for journey stage prediction\n",
        "            X_journey, y_journey, journey_features = self.prepare_features(df, 'journey_stage')\n",
        "            if X_journey is None:\n",
        "                raise Exception(\"Failed to prepare journey stage features\")\n",
        "            \n",
        "            # Prepare features for conversion prediction\n",
        "            X_conversion, y_conversion, conversion_features = self.prepare_features(df, 'conversion')\n",
        "            if X_conversion is None:\n",
        "                raise Exception(\"Failed to prepare conversion features\")\n",
        "            \n",
        "            # Train journey stage model\n",
        "            journey_model, journey_scaler, journey_metrics = self.train_journey_stage_model(X_journey, y_journey)\n",
        "            \n",
        "            # Train conversion model\n",
        "            conversion_model, conversion_scaler, conversion_metrics = self.train_conversion_model(X_conversion, y_conversion)\n",
        "            \n",
        "            # Save models\n",
        "            output_dir = self.save_models(\n",
        "                journey_model, journey_scaler, conversion_model, conversion_scaler,\n",
        "                journey_metrics, conversion_metrics, {\n",
        "                    'journey_stage': journey_features,\n",
        "                    'conversion': conversion_features\n",
        "                }\n",
        "            )\n",
        "            \n",
        "            logger.info(\"=\" * 60)\n",
        "            logger.info(\"üéâ JOURNEY SIMULATION TRAINING COMPLETED!\")\n",
        "            logger.info(\"=\" * 60)\n",
        "            logger.info(f\"üìä Trained 2 models on {len(df)} customers\")\n",
        "            logger.info(f\"üîß Journey Stage features: {len(journey_features)}, Conversion features: {len(conversion_features)}\")\n",
        "            logger.info(f\"üíæ Models saved to: {output_dir}\")\n",
        "            \n",
        "            return {\n",
        "                'journey_stage': journey_model,\n",
        "                'conversion': conversion_model,\n",
        "                'journey_metrics': journey_metrics,\n",
        "                'conversion_metrics': conversion_metrics\n",
        "            }\n",
        "            \n",
        "        except Exception as e:\n",
        "            logger.error(f\"‚ùå Error in training pipeline: {e}\")\n",
        "            raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:üöÄ Starting Journey Simulation Training Pipeline...\n",
            "INFO:__main__:ÔøΩÔøΩ Loading historical training data from S3...\n",
            "INFO:utils.s3_utils:Loading recent inference data from S3 (last 3 months)...\n",
            "INFO:utils.s3_utils:Loading data newer than 2025-06-03\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output//unified_customer_dataset.parquet to data_pipelines/unified_dataset/output/unified_customer_dataset.parquet\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output//unified_dataset_report.yaml to data_pipelines/unified_dataset/output/unified_dataset_report.yaml\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output//unified_dataset_summary.yaml to data_pipelines/unified_dataset/output/unified_dataset_summary.yaml\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output/recent_customer_dataset.parquet to data_pipelines/unified_dataset/output/recent_customer_dataset.parquet\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output/timeline_datasets_metadata.yaml to data_pipelines/unified_dataset/output/timeline_datasets_metadata.yaml\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output/unified_customer_dataset.parquet to data_pipelines/unified_dataset/output/unified_customer_dataset.parquet\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output/unified_dataset_report.yaml to data_pipelines/unified_dataset/output/unified_dataset_report.yaml\n",
            "INFO:utils.s3_utils:Downloading s3://nuscale-data-services-public/amato_pm/data_pipelines/unified_dataset/output/unified_dataset_summary.yaml to data_pipelines/unified_dataset/output/unified_dataset_summary.yaml\n",
            "INFO:__main__:‚úÖ Historical training data loaded from S3\n",
            "INFO:__main__:‚úÖ Loaded historical training dataset: (8460, 89)\n",
            "INFO:__main__:ÔøΩÔøΩ Preparing features for journey_stage prediction...\n",
            "INFO:__main__:‚úÖ Prepared 8460 customers with 5 features for journey_stage\n",
            "INFO:__main__:ÔøΩÔøΩ Preparing features for conversion prediction...\n",
            "INFO:__main__:‚úÖ Prepared 8460 customers with 5 features for conversion\n",
            "INFO:__main__:üéØ Training journey stage prediction model...\n",
            "INFO:__main__:‚úÖ Journey stage prediction training completed\n",
            "INFO:__main__:   Accuracy: 0.9823\n",
            "INFO:__main__:   F1 Score: 0.9814\n",
            "INFO:__main__:   CV Accuracy: 0.9811 (+/- 0.0029)\n",
            "INFO:__main__:üéØ Training conversion prediction model...\n",
            "INFO:__main__:‚úÖ Conversion prediction training completed\n",
            "INFO:__main__:   Accuracy: 0.9994\n",
            "INFO:__main__:   F1 Score: 0.9994\n",
            "INFO:__main__:   CV Accuracy: 1.0000 (+/- 0.0000)\n",
            "INFO:__main__:üíæ Saving trained models...\n",
            "INFO:utils.s3_utils:Uploading models/journey_simulation/journey_stage_model.pkl to s3://nuscale-data-services-public/amato_pm/amato_pm/models/journey_simulation/journey_stage_model.pkl\n",
            "INFO:utils.s3_utils:Successfully uploaded models/journey_simulation/journey_stage_model.pkl\n",
            "INFO:utils.s3_utils:Uploading models/journey_simulation/journey_stage_scaler.pkl to s3://nuscale-data-services-public/amato_pm/amato_pm/models/journey_simulation/journey_stage_scaler.pkl\n",
            "INFO:utils.s3_utils:Successfully uploaded models/journey_simulation/journey_stage_scaler.pkl\n",
            "INFO:utils.s3_utils:Uploading models/journey_simulation/conversion_prediction_model.pkl to s3://nuscale-data-services-public/amato_pm/amato_pm/models/journey_simulation/conversion_prediction_model.pkl\n",
            "INFO:utils.s3_utils:Successfully uploaded models/journey_simulation/conversion_prediction_model.pkl\n",
            "INFO:utils.s3_utils:Uploading models/journey_simulation/conversion_prediction_scaler.pkl to s3://nuscale-data-services-public/amato_pm/amato_pm/models/journey_simulation/conversion_prediction_scaler.pkl\n",
            "INFO:utils.s3_utils:Successfully uploaded models/journey_simulation/conversion_prediction_scaler.pkl\n",
            "INFO:utils.s3_utils:Uploading models/journey_simulation/pipeline_report.yaml to s3://nuscale-data-services-public/amato_pm/amato_pm/models/journey_simulation/pipeline_report.yaml\n",
            "INFO:utils.s3_utils:Successfully uploaded models/journey_simulation/pipeline_report.yaml\n",
            "INFO:__main__:‚úÖ Models uploaded to S3\n",
            "INFO:__main__:‚úÖ Models saved to models/journey_simulation\n",
            "INFO:__main__:============================================================\n",
            "INFO:__main__:üéâ JOURNEY SIMULATION TRAINING COMPLETED!\n",
            "INFO:__main__:============================================================\n",
            "INFO:__main__:üìä Trained 2 models on 8460 customers\n",
            "INFO:__main__:üîß Journey Stage features: 5, Conversion features: 5\n",
            "INFO:__main__:üíæ Models saved to: models/journey_simulation\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üéâ Journey Simulation Training completed successfully!\n",
            "üìä Journey Stage: Accuracy = 0.9823, CV Accuracy = 0.9811\n",
            "üìä Conversion: Accuracy = 0.9994, CV Accuracy = 1.0000\n",
            "üíæ Models saved and ready for inference!\n"
          ]
        }
      ],
      "source": [
        "# Initialize and run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    pipeline = JourneySimulationPipeline()\n",
        "    results = pipeline.run_training_pipeline()\n",
        "    \n",
        "    print(\"\\nüéâ Journey Simulation Training completed successfully!\")\n",
        "    print(f\"üìä Journey Stage: Accuracy = {results['journey_metrics']['accuracy']:.4f}, CV Accuracy = {results['journey_metrics']['cv_accuracy_mean']:.4f}\")\n",
        "    print(f\"üìä Conversion: Accuracy = {results['conversion_metrics']['accuracy']:.4f}, CV Accuracy = {results['conversion_metrics']['cv_accuracy_mean']:.4f}\")\n",
        "    print(\"üíæ Models saved and ready for inference!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
